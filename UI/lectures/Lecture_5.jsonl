{"text": "the tech world is a buzz with hype about gpt-3 model, that is doing impressive, things, with their abilities, Android is, is this one of a website where you can type a description of a, a web application and that description to be sent to juvie, c303 would actually generate code and build decks website for you with a simple tiny react application, but I thought this was as far as its language models and in machine learning, Involved. Another interesting application is this that uses GP Q3 inside of a spreadsheet?\n"}
{"text": "And so it was able to figure out the relationship. But notice that this is not a calculation. It's actually going and getting that piece of information will that piece of information is actually included inside of GPT 3. So I thought this was really impressive.\n"}
{"text": "You can find multiple examples in demos and they will keep rolling in in the next days and weeks of things that Stevie T3 and massive language models. Like he is actually to an API served by openai company. In in API, is in invite-only Baytown, how so you apply, and they give you access, and then you were able to send Franz and put Braum's in interact with and then Inside that API on this Earth of openai is a machine learning model called and that is the model that we will be talking about.\n"}
{"text": "I want to remove the air of mystery and explain some of the main concept behind how a model such as this is built and trained and that's what we will be talking about this. So we can see the opening as a model, as a black box that takes an input a series or a sequence of words. And it generates an object, which is another sequence of words is an example from the foundation and the work of Isaac Asimov for the three laws of robotics. Use this show at the simplest way to think of it as a machine learning model, that takes after his sentence or text and outputs and then they output is generated using what the model has learned during IT training face. The training phase. Do we start with an untrained Mal? And in this case, the model was drained against a lot of data, 300 billion tokens, a lot of tax collected from the internet. Train, I want a specific task against all of this ticket. What is the task that the model was drained on? It is predicting the next one. So we present a few words to the mall. I speak with the word. Then we say, Okay, predict the next one and the model has to do that and it will after millions of steps to learn something. And it captures something from the probability distributions in relationships is out of those examples of texts and we'll look at that in this next week. So how do we generate training examples from the text? So let's say we have this sentence at the top of the second law of Robotics. A robot must obey the orders given it by human beings. We can generate multiple examples that we trained them against so we can take four or five of these words as important. We say, okay to the 6th work, which is a in this case. Give you six words and you generate to be seventh forums and websites and newspapers. You'll end up with a lot of texts that you can break it down like this and generates millions and billions of examples for the AI to be able to be able to learn from. This is a one step of the training process that we just discussed. So we have the example that we generated. So we know that a robot must obey represent, the model with only those three words and green, a robot. And we say, OK, predicts the next to her. We don't show it the actual answer but we know what that we bringing this these examples to the mall. The model makes a calculation. We'll talk later about how that calculation is made at the Highland and it generates and outputs and that output is not going. A good opportunity. This model is not yet. Fully trained untrained or it's starting to go through the training process. It's always you generated an output of a words that is droll incorrect. The correct answer. We were expecting is obey. We have a way of calculating the air. We have, we can quantify how much this prediction was wrong or how much was off. And then that's area calculation is fed back into the model where we update the weights, are parameters of the model where we updated the model works. So that the next time it's makes it comes across this example. Its able to generate a bag, a better prediction, a prediction that is much closer to him than it is to step up for training Machinery. Mom's novel on a lot of which will discuss here is an explanation of how everything works. Not what they did invented or what is the novel in this month? If you've come across machine learning example, before this is the loop that is usually used in in in supervisor training.\n"}
{"text": "I'll talk to you in a little bit more detail. And so the model actually, takes each token, reach word at a time and outputs, its output. Also one token at a time. I just mentioned that this video, we're just discussing how the model works and not what is the novel or what is new in the main thing. That is actually Mew in in the jacuzzi. Three is this size model is massive. What is the size of a model? The model contains? A lot of numbers called parameters or weights in DVD. 3D contains 175 billion of these. Now you can look at my first video of the into my intro to Ai and in that we look at a simple machine learning model with one. Weights one parameter that you can make predictions using. Now. This is some of the latest high-tech models and it's using 175 and these are numbers that the model uses to encode. What it's learned from being exposed to all of this time and I refer you to my interest. And these models, these these parameters are sorted into various matrices inside the bottle. And the process of generating, a prediction is mostly multiplying these different matrices together by the input to the tamale. That's each word. Each token flows through a truck and the model has a context window of 2048 and do the inputs and outputs. How have to fit within that number of tokens in their ways are going to be on that. So you can adjust the model of doing to do more than that number of tokens, but for all we understand right now or one good way of to start to understand how a Transformer model life, to be three words into think about the number of 2. Is it in each of these token? This is processed on its own and its own track. And then once we've processed all of the info, what's the model starts to generates tokens that we can use as Alpha to think of as the outputs of of the month. One way of thinking, how the model works is this? So you have the words and with every word you have a vector representing, that's words. And I would like to refer you to my illustrated words to post on my blog. If you want to understand a little bit more about words in wedding. And in this case, each word has a list of a vector for a list of numbers that capture some of the meaning represent that those are the boxes hearing and yellow and green, and I'll do it. When we process a word. We actually process the vector and that's back there goes through various. Sears of Transformer III has 96, you see how these are are stacked one on top of each other. This is the depth when you hear deep learning learning is is these models that are little bit more complicated, they're able to extract or make predictions that are Olympic more sophisticated using various number of your high number of layers where the computation flows between them and we see that earlier layers process, different things than later and processing of the first token and then the second dokkan goes in and it's it's processed through every layer and then every token in the input sequence goes in. And then when we are processing the last token and then put the outfit with, this is an example of where we're giving a command. To this day I model and then this is its response to a human supposed to say x-ray in into how this model is structured and how it processes. It's important generation examples. Why something is that the model works? Like, this scription is given as an input to the mall, but we also have to give it a number of examples to Prime the model to generate the kind of alka Tutto to let it know that we're expecting react when we gave you this description. And then to do that, we have to give the model two or three or 10, or more examples of description description code. And between the examples, we have special tokens. So there's my assumption of how this works, given.\n"}
{"text": "How would you be? Transformer, Longwood models of words. We don't have an implementation to look at the yet for gpt-3, but this is my pesto is, is able to know we have not seen the best in the coming weeks and months. That's because the model is going to be able to do more amazing things once open releases the ability, or the feature of a being able to fine-tune the mall. And then one of the tools in a large machine, learning with a line with models, and other model has enabled. Some, some of the really impressive results gpt-3, as we've discussed. Using the same mom, the same weight that were trained and costs whatever $5000000 is 4.6 billion dollars to to to to train and 355. Years of your GPU time if it's processing 1 GT. So that training process has been done and then every demo. We've seen so far use that one model with no updates to the wait, just changes in the prom to the input to be able to get them all to do more interesting. Fine-tuning is something that's going to be rolled out. I believe soon, heard from from Albany, I end in that case, you give them all to give over the air TV. The API more examples in the mom is actually trained a little bit more and the weights are dated. So the model is able to create better websites will do better translation from one language to another and it's really start to see some really impressive them was once this So this is it.\n"}
{"text": "This is a high-level overview of how dp3 Works.\n"}
{"text": "Hope you enjoyed it. Please let me know. If you have any ideas or comments, Please Subscribe. And like and video.\n"}
