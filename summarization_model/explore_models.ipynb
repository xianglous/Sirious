{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Explore models\n",
    "Try SOTA text summarization models for their performance on speech/lecture transcripts\n",
    "Since Sat. Oct. 23rd, 2021\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "## Setup\n",
    "import torch\n",
    "# import tensorflow\n",
    "from transformers import pipeline\n",
    "\n",
    "from util import *\n",
    "\n",
    "\n",
    "txt = get_ted_eg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## BigBird\n",
    "\"Big Bird: Transformers for Longer Sequences\".\n",
    "Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n",
    "*NeurIPS 2020*.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| bigbird_pegasus(txt): ['<s> this is an example of what a supervised learning model looks like.<n> '\n",
      "                           'this is an example of what a supervised learning model looks like.<n> this '\n",
      "                           'is an example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.<n> this is an '\n",
      "                           'example of what a supervised learning model looks like.']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['<s> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.<n> this is an example of what a supervised learning model looks like.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distill_bart(text):\n",
    "    \"\"\"\n",
    "    Default: `sshleifer/distilbart-cnn-12-6` in pytorch\n",
    "    \"\"\"\n",
    "    summarizer = pipeline('summarization')\n",
    "    summarizer(text, min_length=50, max_length=200)  # Too much tokens than maximum of 1024\n",
    "\n",
    "\n",
    "def t5(text):\n",
    "    \"\"\"\n",
    "    `t5` in tf\n",
    "    \"\"\"\n",
    "    summarizer = pipeline('summarization', model='t5-base', tokenizer='t5-base', framework='tf')\n",
    "    summarizer(text, min_length=5, max_length=20)\n",
    "\n",
    "\n",
    "def bigbird(text):\n",
    "    \"\"\"\n",
    "    `BigBird` in torch\n",
    "\n",
    "    Looks like doesn't suppport summarization\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\n",
    "        'summarization',\n",
    "        model='google/bigbird-roberta-base',\n",
    "        tokenizer='google/bigbird-roberta-base'\n",
    "    )\n",
    "    summarizer(text, min_length=5, max_length=20)\n",
    "\n",
    "\n",
    "# bigbird()\n",
    "\n",
    "\n",
    "def bigbird_pegasus(text):\n",
    "    from transformers import PegasusTokenizer, BigBirdPegasusForConditionalGeneration, BigBirdPegasusConfig\n",
    "    model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "    tokenizer = PegasusTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "    inputs = tokenizer([text], max_length=4096, return_tensors='pt', truncation=True)\n",
    "    # return inputs['input_ids']\n",
    "    # print(text[:1000])\n",
    "    # summary_ids = model.generate(inputs['input_ids'], num_beams=8, max_length=512, early_stopping=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=8, max_length=512)\n",
    "    # return [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "    return tokenizer.batch_decode(summary_ids)\n",
    "\n",
    "\n",
    "def bigbird_pegasus_(text):\n",
    "    from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
    "    # by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64\n",
    "    # model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "    model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
    "    # model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    # return inputs['input_ids']\n",
    "    # prediction = model.generate(**inputs)\n",
    "    prediction = model.generate(inputs['input_ids'], num_beams=4)\n",
    "    return tokenizer.batch_decode(prediction)\n",
    "\n",
    "\n",
    "# Why do they produce drastically different outputs, with only hyper-parameter difference?\n",
    "# Produces\n",
    "# '<s> we present a brief discussion of the nature of education in the era of '\n",
    "# 'big data and big rip.<n> we start with a brief history of education in the '\n",
    "# 'era of big rip.<n> we then turn to a brief discussion of the nature of '\n",
    "# 'education in the era of big rip.<n> we conclude with a discussion of the '\n",
    "# 'future of education.'\n",
    "# i1 = bigbird_pegasus(txt)\n",
    "# ic(i1)\n",
    "# i2 = bigbird_pegasus_(txt)\n",
    "# ic(i1)\n",
    "# assert i1.equal(i2)\n",
    "\n",
    "\n",
    "\n",
    "txt = get_498_eg(section=True)\n",
    "# txt = get_1st_n_words(txt)\n",
    "# ic(txt)\n",
    "ic(bigbird_pegasus(txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}