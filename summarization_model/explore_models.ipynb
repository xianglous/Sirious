{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Explore models\n",
    "Try SOTA text summarization models for their performance on speech/lecture transcripts\n",
    "Since Sat. Oct. 23rd, 2021\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(txt_ted.split()): 3170\n",
      "ic| len(txt_498.split()): 10548\n",
      "ic| len(txt_498_sec.split()): 3561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Good morning. How are you? \n",
      " (Audience) Good. \n",
      " It's been great, hasn't it? I've been blown away by the whole thing. In fact, I'm leaving. \n",
      " (Laughter) \n",
      " There have been three themes running through the conference, which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we've had and in all of the people here; just the variety of it and the range of it. The second is that it's put us in a place where we have no idea what's going to happen in terms of the future. No idea how this may play out. \n",
      " I have an interest in education. Actually, what I find is, everybody has an interest in education. Don't you? I find this very interesting. If you're at a dinner party, and you say you work in education - actually, you're not often at dinner parties, frankly. \n",
      " (Laughter) \n",
      " If you work in education, you're not asked. \n",
      " (Laughter) \n",
      " And you're never asked back, curiously. That's strange to me. But if you are, and you say to somebody, you know, they say, \"What do you do?\" and you say you work in education, you can see the blood run from their face. They're like, \"Oh my God. Why me?\" \n",
      " (Laughter) \n",
      " \"My one night out all week.\" \n",
      " (Laughter) \n",
      " But if you ask about their education, they pin you to the wall, because it's one of those things that goes deep with people, am I right? Like religion and money and other things. So I have a big interest in education, and I think we all do. We have a huge vested interest in it, partly because it's education that's meant to take us into this future that we can't grasp. If you think of it, children starting school this year will be retiring in 2065. Nobody has a clue, despite all the expertise that's been on parade for the past four days, what the world will look like in five years' time. And yet, we're meant to be educating them for it. So the unpredictability, I think, is extraordinary. \n",
      " And the third part of this is that we've all agreed, nonetheless, on the really extraordinary capacities that children have - their capacities for innovation. I mean, Sirena last night was a marvel, wasn't she? Just seeing what she could do. And she's exceptional, but I think she's not, so to speak, exceptional in the whole of childhood. What you have there is a person of extraordinary dedication who found a talent. And my contention is, all kids have tremendous talents, and we squander them, pretty ruthlessly. \n",
      " So I want to talk about education, and I want to talk about creativity. My contention is that creativity now is as important in education as literacy, and we should treat it with the same status. \n",
      " (Applause) \n",
      " Thank you. \n",
      " (Applause) \n",
      " That was it, by the way. Thank you very much. \n",
      " (Laughter) \n",
      " So, 15 minutes left. \n",
      " (Laughter) \n",
      " \"Well, I was born ... \" \n",
      " (Laughter) \n",
      " I heard a great story recently - I love telling it - of a little girl who was in a drawing lesson. She was six, and she was at the back, drawing, and the teacher said this girl hardly ever paid attention, and in this drawing lesson, she did. The teacher was fascinated. She went over to her, and she said, \"What are you drawing?\" And the girl said, \"I'm drawing a picture of God.\" And the teacher said, \"But nobody knows what God looks like.\" And the girl said, \"They will in a minute.\" \n",
      " (Laughter) \n",
      " When my son was four in England - actually, he was four everywhere, to be honest. \n",
      " (Laughter) \n",
      " If we're being strict about it, wherever he went, he was four that year. He was in the Nativity play. Do you remember the story? \n",
      " (Laughter) \n",
      " No, it was big, it was a big story. Mel Gibson did the sequel, you may have seen it. \n",
      " (Laughter) \n",
      " \"Nativity II.\" But James got the part of Joseph, which we were thrilled about. We considered this to be one of the lead parts. We had the place crammed full of agents in T-shirts: \"James Robinson IS Joseph!\" (Laughter) He didn't have to speak, but you know the bit where the three kings come in? They come in bearing gifts, gold, frankincense and myrrh. This really happened. We were sitting there, and I think they just went out of sequence, because we talked to the little boy afterward and said, \"You OK with that?\" They said, \"Yeah, why? Was that wrong?\" They just switched. The three boys came in, four-year-olds with tea towels on their heads. They put these boxes down, and the first boy said, \"I bring you gold.\" And the second boy said, \"I bring you myrrh.\" And the third boy said, \"Frank sent this.\" \n",
      " (Laughter) \n",
      " What these things have in common is that kids will take a chance. If they don't know, they'll have a go. Am I right? They're not frightened of being wrong. I don't mean to say that being wrong is the same thing as being creative. What we do know is, if you're not prepared to be wrong, you'll never come up with anything original - if you're not prepared to be wrong. And by the time they get to be adults, most kids have lost that capacity. They have become frightened of being wrong. And we run our companies like this. We stigmatize mistakes. And we're now running national education systems where mistakes are the worst thing you can make. And the result is that we are educating people out of their creative capacities. \n",
      " Picasso once said this, he said that all children are born artists. The problem is to remain an artist as we grow up. I believe this passionately, that we don't grow into creativity, we grow out of it. Or rather, we get educated out of it. So why is this? \n",
      " I lived in Stratford-on-Avon until about five years ago. In fact, we moved from Stratford to Los Angeles. So you can imagine what a seamless transition this was. \n",
      " (Laughter) \n",
      " Actually, we lived in a place called Snitterfield, just outside Stratford, which is where Shakespeare's father was born. Are you struck by a new thought? I was. You don't think of Shakespeare having a father, do you? Do you? Because you don't think of Shakespeare being a child, do you? Shakespeare being seven? I never thought of it. I mean, he was seven at some point. He was in somebody's English class, wasn't he? \n",
      " (Laughter) \n",
      " How annoying would that be? \n",
      " (Laughter) \n",
      " \"Must try harder.\" \n",
      " (Laughter) \n",
      " Being sent to bed by his dad, to Shakespeare, \"Go to bed, now!\" To William Shakespeare. \"And put the pencil down!\" \n",
      " (Laughter) \n",
      " \"And stop speaking like that.\" \n",
      " (Laughter) \n",
      " \"It's confusing everybody.\" \n",
      " (Laughter) \n",
      " Anyway, we moved from Stratford to Los Angeles, and I just want to say a word about the transition. Actually, my son didn't want to come. I've got two kids; he's 21 now, my daughter's 16. He didn't want to come to Los Angeles. He loved it, but he had a girlfriend in England. This was the love of his life, Sarah. He'd known her for a month. \n",
      " (Laughter) \n",
      " Mind you, they'd had their fourth anniversary, because it's a long time when you're 16. He was really upset on the plane. He said, \"I'll never find another girl like Sarah.\" And we were rather pleased about that, frankly - \n",
      " (Laughter) \n",
      " because she was the main reason we were leaving the country. \n",
      " (Laughter) \n",
      " But something strikes you when you move to America and travel around the world: every education system on earth has the same hierarchy of subjects. Every one. Doesn't matter where you go. You'd think it would be otherwise, but it isn't. At the top are mathematics and languages, then the humanities. At the bottom are the arts. Everywhere on earth. And in pretty much every system, too, there's a hierarchy within the arts. Art and music are normally given a higher status in schools than drama and dance. There isn't an education system on the planet that teaches dance every day to children the way we teach them mathematics. Why? Why not? I think this is rather important. I think math is very important, but so is dance. Children dance all the time if they're allowed to, we all do. We all have bodies, don't we? Did I miss a meeting? \n",
      " (Laughter) \n",
      " Truthfully, what happens is, as children grow up, we start to educate them progressively from the waist up. And then we focus on their heads. And slightly to one side. \n",
      " If you were to visit education as an alien and say \"What's it for, public education?\" I think you'd have to conclude, if you look at the output, who really succeeds by this, who does everything they should, who gets all the brownie points, who are the winners - I think you'd have to conclude the whole purpose of public education throughout the world is to produce university professors. Isn't it? They're the people who come out the top. And I used to be one, so there. \n",
      " (Laughter) \n",
      " And I like university professors, but, you know, we shouldn't hold them up as the high-water mark of all human achievement. They're just a form of life. Another form of life. But they're rather curious. And I say this out of affection for them: there's something curious about professors. In my experience - not all of them, but typically - they live in their heads. They live up there and slightly to one side. They're disembodied, you know, in a kind of literal way. They look upon their body as a form of transport for their heads. \n",
      " (Laughter) \n",
      " Don't they? It's a way of getting their head to meetings. \n",
      " (Laughter) \n",
      " If you want real evidence of out-of-body experiences, by the way, get yourself along to a residential conference of senior academics and pop into the discotheque on the final night. \n",
      " (Laughter) \n",
      " And there, you will see it. Grown men and women writhing uncontrollably, off the beat. \n",
      " (Laughter) \n",
      " Waiting until it ends, so they can go home and write a paper about it. \n",
      " (Laughter) \n",
      " Our education system is predicated on the idea of academic ability. And there's a reason. Around the world, there were no public systems of education, really, before the 19th century. They all came into being to meet the needs of industrialism. So the hierarchy is rooted on two ideas. \n",
      " Number one, that the most useful subjects for work are at the top. So you were probably steered benignly away from things at school when you were a kid, things you liked, on the grounds you would never get a job doing that. Is that right? \"Don't do music, you're not going to be a musician; don't do art, you won't be an artist.\" Benign advice - now, profoundly mistaken. The whole world is engulfed in a revolution. \n",
      " And the second is academic ability, which has really come to dominate our view of intelligence, because the universities design the system in their image. If you think of it, the whole system of public education around the world is a protracted process of university entrance. And the consequence is that many highly talented, brilliant, creative people think they're not, because the thing they were good at at school wasn't valued, or was actually stigmatized. And I think we can't afford to go on that way. \n",
      " In the next 30 years, according to UNESCO, more people worldwide will be graduating through education than since the beginning of history. More people. And it's the combination of all the things we've talked about: technology and its transformational effect on work, and demography and the huge explosion in population. \n",
      " Suddenly, degrees aren't worth anything. Isn't that true? When I was a student, if you had a degree, you had a job. If you didn't have a job, it's because you didn't want one. And I didn't want one, frankly. \n",
      " (Laughter) \n",
      " But now kids with degrees are often heading home to carry on playing video games, because you need an MA where the previous job required a BA, and now you need a PhD for the other. It's a process of academic inflation. And it indicates the whole structure of education is shifting beneath our feet. We need to radically rethink our view of intelligence. \n",
      " We know three things about intelligence. One, it's diverse. We think about the world in all the ways that we experience it. We think visually, we think in sound, we think kinesthetically. We think in abstract terms, we think in movement. Secondly, intelligence is dynamic. If you look at the interactions of a human brain, as we heard yesterday from a number of presentations, intelligence is wonderfully interactive. The brain isn't divided into compartments. In fact, creativity - which I define as the process of having original ideas that have value - more often than not comes about through the interaction of different disciplinary ways of seeing things. \n",
      " By the way, there's a shaft of nerves that joins the two halves of the brain, called the corpus callosum. It's thicker in women. Following off from Helen yesterday, this is probably why women are better at multitasking. Because you are, aren't you? There's a raft of research, but I know it from my personal life. If my wife is cooking a meal at home, which is not often ... thankfully. \n",
      " (Laughter) \n",
      " No, she's good at some things. But if she's cooking, she's dealing with people on the phone, she's talking to the kids, she's painting the ceiling - \n",
      " (Laughter) \n",
      " she's doing open-heart surgery over here. If I'm cooking, the door is shut, the kids are out, the phone's on the hook, if she comes in, I get annoyed. I say, \"Terry, please, I'm trying to fry an egg in here.\" \n",
      " (Laughter) \n",
      " \"Give me a break.\" \n",
      " (Laughter) \n",
      " Actually, do you know that old philosophical thing, \"If a tree falls in a forest, and nobody hears it, did it happen?\" Remember that old chestnut? I saw a great T-shirt recently, which said, \"If a man speaks his mind in a forest, and no woman hears him, is he still wrong?\" \n",
      " (Laughter) \n",
      " And the third thing about intelligence is, it's distinct. I'm doing a new book at the moment called \"Epiphany,\" which is based on a series of interviews with people about how they discovered their talent. I'm fascinated by how people got to be there. It's really prompted by a conversation I had with a wonderful woman who maybe most people have never heard of, Gillian Lynne. Have you heard of her? Some have. She's a choreographer, and everybody knows her work. She did \"Cats\" and \"Phantom of the Opera.\" She's wonderful. I used to be on the board of The Royal Ballet, as you can see. \n",
      " (Laughter) \n",
      " Gillian and I had lunch one day. I said, \"How did you get to be a dancer?\" It was interesting. When she was at school, she was really hopeless. And the school, in the '30s, wrote to her parents and said, \"We think Gillian has a learning disorder.\" She couldn't concentrate; she was fidgeting. I think now they'd say she had ADHD. Wouldn't you? But this was the 1930s, and ADHD hadn't been invented at this point. It wasn't an available condition. \n",
      " (Laughter) \n",
      " People weren't aware they could have that. \n",
      " (Laughter) \n",
      " Anyway, she went to see this specialist. So, this oak-paneled room, and she was there with her mother, and she was led and sat on this chair at the end, and she sat on her hands for 20 minutes, while this man talked to her mother about all the problems Gillian was having at school, because she was disturbing people, her homework was always late, and so on. Little kid of eight. In the end, the doctor went and sat next to Gillian and said, \"I've listened to all these things your mother's told me. I need to speak to her privately. Wait here. We'll be back. We won't be very long,\" and they went and left her. \n",
      " But as they went out of the room, he turned on the radio that was sitting on his desk. And when they got out of the room, he said to her mother, \"Just stand and watch her.\" And the minute they left the room, she was on her feet, moving to the music. And they watched for a few minutes, and he turned to her mother and said, \"Mrs. Lynne, Gillian isn't sick. She's a dancer. Take her to a dance school.\" \n",
      " I said, \"What happened?\" She said, \"She did. I can't tell you how wonderful it was. We walked in this room, and it was full of people like me - people who couldn't sit still, people who had to move to think.\" Who had to move to think. They did ballet, they did tap, jazz; they did modern; they did contemporary. She was eventually auditioned for the Royal Ballet School. She became a soloist; she had a wonderful career at the Royal Ballet. She eventually graduated from the Royal Ballet School, founded the Gillian Lynne Dance Company, met Andrew Lloyd Webber. She's been responsible for some of the most successful musical theater productions in history, she's given pleasure to millions, and she's a multimillionaire. Somebody else might have put her on medication and told her to calm down. \n",
      " (Applause) \n",
      " What I think it comes to is this: Al Gore spoke the other night about ecology and the revolution that was triggered by Rachel Carson. I believe our only hope for the future is to adopt a new conception of human ecology, one in which we start to reconstitute our conception of the richness of human capacity. Our education system has mined our minds in the way that we strip-mine the earth for a particular commodity. And for the future, it won't serve us. We have to rethink the fundamental principles on which we're educating our children. \n",
      " There was a wonderful quote by Jonas Salk, who said, \"If all the insects were to disappear from the Earth, within 50 years, all life on Earth would end. If all human beings disappeared from the Earth, within 50 years, all forms of life would flourish.\" And he's right. \n",
      " What TED celebrates is the gift of the human imagination. We have to be careful now that we use this gift wisely, and that we avert some of the scenarios that we've talked about. And the only way we'll do it is by seeing our creative capacities for the richness they are and seeing our children for the hope that they are. And our task is to educate their whole being, so they can face this future. By the way - we may not see this future, but they will. And our job is to help them make something of it. \n",
      " Thank you very much. \n",
      " (Applause) \n"
     ]
    },
    {
     "data": {
      "text/plain": "3561"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup\n",
    "import torch\n",
    "# import tensorflow\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import PegasusTokenizer, BigBirdPegasusForConditionalGeneration, BigBirdPegasusConfig\n",
    "# import protobuf\n",
    "\n",
    "from util import *\n",
    "\n",
    "\n",
    "txt_ted = get_ted_eg()['transcript']\n",
    "print(txt_ted)\n",
    "ic(len(txt_ted.split()))\n",
    "txt_498 = get_498_eg()\n",
    "txt_498_sec = get_498_eg(section=True)\n",
    "ic(len(txt_498.split()))\n",
    "ic(len(txt_498_sec.split()))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BigBird\n",
    "\"Big Bird: Transformers for Longer Sequences\".\n",
    "Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n",
    "*NeurIPS 2020*.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| txt_498[:400]: (\"No, you don't need a reason. Apple TV. All right. Apple TV. Sweet don't have \"\n",
      "                    \"to carry around a dongle. Welcome, welcome everybody. Yeah, it's um the \"\n",
      "                    \"gloomy day. I got a bunch of great emails saying I can't make it. I feel it. \"\n",
      "                    \"I feel it. But anyway, let's just jump right into how many people have \"\n",
      "                    \"groups? Awesome. That's all you left. The scene. Pretty much everybody. So \"\n",
      "                    \"we'll will tease out who mad\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/76/fm8fgr4n1p78whqg7x_bd5480000gn/T/ipykernel_2388/3329244962.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;31m# txt = get_1st_n_words(txt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;31m# ic(txt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m \u001B[0mic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbigbird_pegasus\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtxt_ted\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m \u001B[0;31m# ic(bigbird_pegasus(txt_498))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/76/fm8fgr4n1p78whqg7x_bd5480000gn/T/ipykernel_2388/3329244962.py\u001B[0m in \u001B[0;36mbigbird_pegasus\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPegasusTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'google/bigbird-pegasus-large-arxiv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4096\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'pt'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0msummary_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_beams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m256\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mearly_stopping\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_decode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msummary_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \"\"\"\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m    920\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_encoder_decoder\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    921\u001B[0m             \u001B[0;31m# add encoder_outputs to model_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 922\u001B[0;31m             \u001B[0mmodel_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_prepare_encoder_decoder_kwargs_for_generation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    923\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    924\u001B[0m             \u001B[0;31m# set input_ids as decoder_input_ids\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/generation_utils.py\u001B[0m in \u001B[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001B[0;34m(self, input_ids, model_kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0margument\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"decoder_\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0margument\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cross_attn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    416\u001B[0m             }\n\u001B[0;32m--> 417\u001B[0;31m             \u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"encoder_outputs\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mModelOutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mencoder_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodel_kwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1921\u001B[0m                     )\n\u001B[1;32m   1922\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1923\u001B[0;31m                     layer_outputs = encoder_layer(\n\u001B[0m\u001B[1;32m   1924\u001B[0m                         \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1925\u001B[0m                         \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, layer_head_mask, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)\u001B[0m\n\u001B[1;32m   1373\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mself_attn_layer_norm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1374\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1375\u001B[0;31m         self_attention_outputs = self.self_attn(\n\u001B[0m\u001B[1;32m   1376\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1377\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, past_key_value, output_attentions, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask)\u001B[0m\n\u001B[1;32m   1188\u001B[0m             )\n\u001B[1;32m   1189\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1190\u001B[0;31m             self_outputs = self.self(\n\u001B[0m\u001B[1;32m   1191\u001B[0m                 \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mband_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_blocked_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mto_blocked_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_attentions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask, output_attentions)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0mvalue_layer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose_for_scores\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m         context_layer, attention_probs = self.bigbird_block_sparse_attention(\n\u001B[0m\u001B[1;32m    285\u001B[0m             \u001B[0mquery_layer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m             \u001B[0mkey_layer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def bigbird(text):\n",
    "    \"\"\"\n",
    "    `BigBird` in torch\n",
    "\n",
    "    Looks like doesn't support summarization\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\n",
    "        'summarization',\n",
    "        model='google/bigbird-roberta-base',\n",
    "        tokenizer='google/bigbird-roberta-base'\n",
    "    )\n",
    "    summarizer(text, min_length=5, max_length=20)\n",
    "# bigbird('hello world')\n",
    "\n",
    "\n",
    "def bigbird_pegasus(text):\n",
    "    model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "    tokenizer = PegasusTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "    inputs = tokenizer([text], max_length=4096, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=256, early_stopping=True)\n",
    "    return tokenizer.batch_decode(summary_ids)\n",
    "\"\"\"\n",
    "Hyper-parameter seems to affect the result drastically\n",
    "\n",
    "On TED.com `Do schools kill creativity?`\n",
    "['<s> we present a brief discussion of the nature of education in the era of '\n",
    "'big data and big rip.<n> we start with a brief history of education in the '\n",
    "'era of big rip.<n> we then turn to a brief discussion of the nature of '\n",
    "'education in the era of big rip.<n> we conclude with a discussion of the '\n",
    "'future of education.']\n",
    "\n",
    "['<s> this is an elementary introduction to the notion of the distance between '\n",
    "'two points on the unit circle.<n> the distance is defined as the difference '\n",
    "'between the value of the unit circle and the value of the point on the unit '\n",
    "'circle.<n> the definition of the distance is based on the concept of an '\n",
    "'equivalence between the value of the unit circle and the value of the point '\n",
    "'on the unit circle.<n> the distance is defined as the difference between the '\n",
    "'value of the unit circle and the value of the point on the unit circle.<n> '\n",
    "'the definition of the distance is based on the concept of an equivalence '\n",
    "'between the value of the unit circle and the value of the point on the unit '\n",
    "'circle. <n> [ [ section ] ] this is an elementary introduction to the notion '\n",
    "'of the distance between two points on the unit circle.<n> the distance is '\n",
    "'defined as the difference between the value of the unit circle and the value '\n",
    "'of the point on the unit circle.<n> the definition of the distance is based '\n",
    "'on the concept of an equivalence between the value of the unit circle and '\n",
    "'the value of the point on the unit circle.<n> the distance is defined as the '\n",
    "'difference between the value of the unit circle and the value of the point '\n",
    "'on the unit circle ']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ic(txt_498[:400])\n",
    "# txt = get_1st_n_words(txt)\n",
    "# ic(txt)\n",
    "ic(bigbird_pegasus(txt_ted))\n",
    "# ic(bigbird_pegasus(txt_498))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## T5\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "\n",
    "\n",
    "def summarize(text, max_length=150):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    return preds[0]\n",
    "\n",
    "\n",
    "ic(summarize(txt_ted))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distill BART\n",
    "Token size of 1024 is not large enough\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# summarizer = pipeline('summarization', model='sshleifer/distilbart-cnn-12-6')\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
    "ic(summarizer(txt_ted, max_length=300))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}