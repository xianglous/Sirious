{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Explore HuggingFace\n",
    "Since Sat. Oct. 23rd, 2021\n",
    "\n",
    "Understand HuggingFace API\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from icecream import ic\n",
    "import torch\n",
    "import tensorflow\n",
    "from transformers import pipeline\n",
    "\n",
    "from summarization_model.util import *\n",
    "\n",
    "\n",
    "os.chdir('..')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usage examples\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Your min_length is set to 50, but you input_length is only 4. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 200, but you input_length is only 4. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "ic| distill_bart('hello world'): None\n",
      "2021-11-27 20:15:03.419586: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-27 20:15:03.476776: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Your max_length is set to 20, but you input_length is only 5. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "ic| t5('hello world'): None\n"
     ]
    }
   ],
   "source": [
    "def distill_bart(text):\n",
    "    \"\"\"\n",
    "    Default: `sshleifer/distilbart-cnn-12-6` in pytorch\n",
    "    \"\"\"\n",
    "    summarizer = pipeline('summarization')\n",
    "    summarizer(text, min_length=50, max_length=200)  # Too much tokens than maximum of 1024\n",
    "\n",
    "\n",
    "def t5(text):\n",
    "    \"\"\"\n",
    "    `t5` in tf\n",
    "    \"\"\"\n",
    "    summarizer = pipeline('summarization', model='t5-base', tokenizer='t5-base', framework='tf')\n",
    "    summarizer(text, min_length=5, max_length=20)\n",
    "\n",
    "ic(distill_bart('hello world'))\n",
    "ic(t5('hello world'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Is `generate()` same as `__call__`?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "ic| summarizer(text, min_length=5, max_length=20): [{'summary_text': 'google is doing a great job of getting submissions in . '\n",
      "                                                                     \"it's one of\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'summary_text': \"google is doing a great job of getting submissions in . it's one of\"}]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model='t5-base', tokenizer='t5-base', framework='tf')\n",
    "text = get_1st_n_words(get_498_eg(), n=200)\n",
    "ic(summarizer(text, min_length=5, max_length=20))\n",
    "# ic(summarizer.generate(text, min_length=5, max_length=20))  # Results in error\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}